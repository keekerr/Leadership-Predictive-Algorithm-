# Imports and Data Load
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from scipy.stats import ttest_ind
from sklearn.preprocessing import LabelEncoder

employee_data = pd.read_csv(r'C:\Users\admin\OneDrive\Desktop\MIS581 Port folio Data Sets\employee_data.csv')
engagement_data = pd.read_csv(r'C:\Users\admin\OneDrive\Desktop\MIS581 Port folio Data Sets\employee_engagement_survey_data.csv')
recruitment_data = pd.read_csv(r'C:\Users\admin\OneDrive\Desktop\MIS581 Port folio Data Sets\recruitment_data.csv')
training_data = pd.read_csv(r'C:\Users\admin\OneDrive\Desktop\MIS581 Port folio Data Sets\training_and_development_data.csv')

# Check for missing values
print("Missing values in training data:\n", training_data.isnull().sum())
print("Missing values in employee data:\n", employee_data.isnull().sum())
print("Missing values in engagement data:\n", engagement_data.isnull().sum())
print("Missing values in recruitment data:\n", recruitment_data.isnull().sum())

# Handle missing values
training_data = training_data.drop_duplicates()
employee_data = employee_data.drop_duplicates()
engagement_data = engagement_data.drop_duplicates()
recruitment_data = recruitment_data.drop_duplicates()

# Merge the datasets based on 'employee_id'
merged_data = pd.merge(employee_data, training_data, how='left', on='employee_id')

# Descriptive Statistics
print("Descriptive statistics for merged data:\n", merged_data.describe())

# Data Visualization
# Distribution of key variables
plt.figure(figsize=(10, 6))
sns.histplot(merged_data['Performance Score'], kde=True)
plt.title('Distribution of Performance Scores')
plt.show()

# Pair plots to explore relationships
sns.pairplot(merged_data)
plt.show()

# Correlation Analysis
numeric_columns = merged_data.select_dtypes(include=[np.number])

plt.figure(figsize=(12, 8))
correlation_matrix = numeric_columns.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Create 'high_performer' column based on 'Performance Score'
merged_data['high_performer'] = merged_data['Performance Score'].apply(lambda x: 1 if x == 'Exceeds' else 0)

# Identify potential KPIs
plt.figure(figsize=(10, 6))
sns.boxplot(x='high_performer', y='Years of Experience', data=merged_data)
plt.title('Years of Experience by High Performer')
plt.show()

label_encoders = {}
categorical_columns = ['Performance Score']

for col in categorical_columns:
    le = LabelEncoder()
    merged_data[col] = le.fit_transform(merged_data[col])
    label_encoders[col] = le

X = merged_data[['Performance Score', 'Years of Experience', 'Training Duration(Days)']].dropna()
y = merged_data[['high_performer']].dropna()
X, y = X.align(y, join='inner', axis=0)  # Align the data
y = y.values.ravel()  # Flatten the array

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Precision:', precision_score(y_test, y_pred))
print('Recall:', recall_score(y_test, y_pred))
print('F1 Score:', f1_score(y_test, y_pred))
print('Confusion Matrix:\n', confusion_matrix(y_test, y_pred))

# Analyze the impact of training using the merged data
plt.figure(figsize=(10, 6))
sns.boxplot(x='Training Outcome', y='Performance Score', data=merged_data)
plt.title('Performance Score by Training Completion')
plt.show()

# Statistical test
training_scores = merged_data[merged_data['Training Outcome'] == 1]['Performance Score']
no_training_scores = merged_data[merged_data['Training Outcome'] == 0]['Performance Score']
t_stat, p_val = ttest_ind(training_scores, no_training_scores)
print('T-statistic:', t_stat)
print('P-value:', p_val)
